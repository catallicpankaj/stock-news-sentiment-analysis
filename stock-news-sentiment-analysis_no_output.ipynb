{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "EvCcfwuSU-fz",
   "metadata": {
    "id": "EvCcfwuSU-fz"
   },
   "source": [
    "## **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6QR_RHvIVHT2",
   "metadata": {
    "id": "6QR_RHvIVHT2"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pl3dmH-EnJGl",
   "metadata": {
    "id": "pl3dmH-EnJGl"
   },
   "source": [
    "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vn6bbxSwVKl3",
   "metadata": {
    "id": "Vn6bbxSwVKl3"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jCIswL3zobj6",
   "metadata": {
    "id": "jCIswL3zobj6"
   },
   "source": [
    "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
    "\n",
    "As a member of the Data Science and AI team in the startup, you have been tasked with developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZJOtDHVSF5hu",
   "metadata": {
    "id": "ZJOtDHVSF5hu"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZlkjI8V5F9RK",
   "metadata": {
    "id": "ZlkjI8V5F9RK"
   },
   "source": [
    "* `Date` : The date the news was released\n",
    "* `News` : The content of news articles that could potentially affect the company's stock price\n",
    "* `Open` : The stock price (in \\$) at the beginning of the day\n",
    "* `High` : The highest stock price (in \\$) reached during the day\n",
    "* `Low` :  The lowest stock price (in \\$) reached during the day\n",
    "* `Close` : The adjusted stock price (in \\$) at the end of the day\n",
    "* `Volume` : The number of shares traded during the day\n",
    "* `Label` : The sentiment polarity of the news content\n",
    "    * 1: positive\n",
    "    * 0: neutral\n",
    "    * -1: negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VrFQHcW5mYgv",
   "metadata": {
    "id": "VrFQHcW5mYgv"
   },
   "source": [
    "## **Installing and Importing the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A-E2-iaumpo8",
   "metadata": {
    "id": "A-E2-iaumpo8"
   },
   "outputs": [],
   "source": [
    "# installing the sentence-transformers and gensim libraries for word embeddings\n",
    "!pip3 install numpy==1.26.4 \\\n",
    "             scikit-learn==1.6.1 \\\n",
    "             scipy==1.13.1 \\\n",
    "             gensim==4.3.3 \\\n",
    "             sentence-transformers==3.4.1 \\\n",
    "             pandas==2.2.2 \\\n",
    "             matplotlib==3.9.2 \\\n",
    "             tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Su4_EiqL5aIZ",
   "metadata": {
    "id": "Su4_EiqL5aIZ"
   },
   "source": [
    "Note:\n",
    "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
    "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U3bnkPu_ZGbE",
   "metadata": {
    "id": "U3bnkPu_ZGbE"
   },
   "outputs": [],
   "source": [
    "# To manipulate and analyze data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%pip install matplotlib seaborn\n",
    "\n",
    "# To visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To used time-related functions\n",
    "import time\n",
    "\n",
    "# To build, tune, and evaluate ML models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# To load/create word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# To work with transformer models\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "%pip install tensorflow\n",
    "\n",
    "# Import TensorFlow and Keras for deep learning model building.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# To implement progress bar related functionalities\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# To ignore unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wQ46zPgumfjF",
   "metadata": {
    "id": "wQ46zPgumfjF"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1xFSwCCer1uA",
   "metadata": {
    "id": "1xFSwCCer1uA"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"\\n=== Loading the Dataset (CSV)===\\n\")\n",
    "import os\n",
    "csv_path = os.path.join(os.getcwd(), 'stock_news.csv')\n",
    "print('Reading file:', csv_path)\n",
    "df_original = pd.read_csv(csv_path)\n",
    "print(f'Data loaded successfully, dataset shape: {df_original.shape}')\n",
    "\n",
    "# Copying the original dataframe to a new variable for safety\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EvFNfrvGWthn",
   "metadata": {
    "id": "EvFNfrvGWthn"
   },
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HEtn1BMJgu83",
   "metadata": {
    "id": "HEtn1BMJgu83"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== DATASET OVERVIEW ===\\n\")\n",
    "# Display shape\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\\n\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "display(df.tail())\n",
    "\n",
    "#Data types\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== DESCRIPTIVE STATISTICS ===\")\n",
    "display(df.describe())\n",
    "\n",
    "# Info about dataset\n",
    "print(\"\\n=== DATASET INFO ===\")\n",
    "df.info()\n",
    "\n",
    "# Check unique values in Label column\n",
    "print(\"\\n=== TARGET VARIABLE ANALYSIS ===\")\n",
    "print(f\"Unique labels: {df['Label'].unique()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"\\nLabel distribution (%):\")\n",
    "print(df['Label'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hGHBK8-QeKOB",
   "metadata": {
    "id": "hGHBK8-QeKOB"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Convert Date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Create additional features for analysis\n",
    "df['News_Length'] = df['News'].str.len()\n",
    "df['Word_Count'] = df['News'].str.split().str.len()\n",
    "df['Price_Range'] = df['High'] - df['Low']\n",
    "df['Price_Change'] = df['Close'].pct_change() * 100\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day_of_Week'] = df['Date'].dt.day_name()\n",
    "\n",
    "print(\"\\nNew features created for analysis:\")\n",
    "print(\"- News_Length: Character count in news\")\n",
    "print(\"- Word_Count: Number of words in news\")\n",
    "print(\"- Price_Range: Daily price range (High - Low)\")\n",
    "print(\"- Price_Change: Percentage change in closing price\")\n",
    "print(df[['News_Length', 'Word_Count', 'Price_Range', 'Price_Change']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q0UlMQnyegl7",
   "metadata": {
    "id": "Q0UlMQnyegl7"
   },
   "source": [
    "### **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9GVt_AAbe29X",
   "metadata": {
    "id": "9GVt_AAbe29X"
   },
   "source": [
    "* Distribution of individual variables\n",
    "* Compute and check the distribution of the length of news content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5932675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- UNIVARIATE ANALYSIS ---\\n\")\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comprehensive univariate visualizations\n",
    "fig = plt.figure(figsize=(24, 14))\n",
    "\n",
    "# 1. Distribution of Sentiment Labels (Count)\n",
    "print(\"# 1. Distribution of Sentiment Labels (Count)...\")\n",
    "plt.subplot(3, 4, 1)\n",
    "sentiment_counts = df['Label'].value_counts().sort_index()\n",
    "colors = ['#ff6b6b', '#95a5a6', '#4ecdc4']\n",
    "bars = plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('Sentiment Label', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Sentiment Labels', fontsize=12, fontweight='bold')\n",
    "plt.xticks([-1, 0, 1], ['Negative', 'Neutral', 'Positive'])\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Distribution of Sentiment Labels (Pie Chart)\n",
    "print(\"# 2. Distribution of Sentiment Labels (Pie Chart)...\")\n",
    "plt.subplot(1,1,1)\n",
    "labels_pie = ['Negative (-1)', 'Neutral (0)', 'Positive (1)']\n",
    "plt.pie(sentiment_counts.values, labels=labels_pie, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "plt.title('Sentiment Label Proportion', fontsize=12, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distribution of Closing Prices\n",
    "print(\"# 3. Distribution of Closing Prices...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['Close'], bins=40, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "plt.axvline(df['Close'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df[\"Close\"].mean():.2f}')\n",
    "plt.axvline(df['Close'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${df[\"Close\"].median():.2f}')\n",
    "plt.xlabel('Closing Price ($)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Closing Prices', fontsize=12, fontweight='bold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c604921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Distribution of Trading Volume\n",
    "print(\"# 4. Distribution of Trading Volume...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['Volume'], bins=40, alpha=0.7, color='#2ecc71', edgecolor='black')\n",
    "plt.xlabel('Trading Volume', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Trading Volume', fontsize=12, fontweight='bold')\n",
    "plt.ticklabel_format(style='scientific', axis='x', scilimits=(0,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf731f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Distribution of News Length\n",
    "print(\"# 5. Distribution of News Length...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['News_Length'], bins=50, alpha=0.7, color='#e74c3c', edgecolor='black')\n",
    "plt.axvline(df['News_Length'].mean(), color='blue', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {df[\"News_Length\"].mean():.0f}')\n",
    "plt.xlabel('News Length (characters)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of News Content Length', fontsize=12, fontweight='bold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33847857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot KDE for the 'Open', 'High', 'Low', 'Close' columns of the 'stock' DataFrame.\n",
    "print(\"# 6. Plot KDE for the 'Open', 'High', 'Low', 'Close' columns of the 'stock' DataFrame...\")\n",
    "sns.displot(data=df[['Open','High','Low','Close']], kind='kde', palette=\"tab10\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plots a histogram of the 'Volume' column\n",
    "print(\"# 7. Plots a histogram of the 'Volume' column...\")\n",
    "sns.histplot(df, x='Volume');  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeed2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Distribution of Word Count\n",
    "print(\"# 8. Distribution of Word Count...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['Word_Count'], bins=50, alpha=0.7, color='#9b59b6', edgecolor='black')\n",
    "plt.axvline(df['Word_Count'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {df[\"Word_Count\"].mean():.0f}')\n",
    "plt.xlabel('Word Count', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Word Count in News', fontsize=12, fontweight='bold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Distribution of Daily Price Range\n",
    "print(\"# 9. Distribution of Daily Price Range...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['Price_Range'], bins=40, alpha=0.7, color='#f39c12', edgecolor='black')\n",
    "plt.xlabel('Price Range ($)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Daily Price Range', fontsize=12, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Distribution of Opening Prices\n",
    "print(\"# 10. Distribution of Opening Prices...\")\n",
    "plt.subplot(1,1,1)\n",
    "plt.hist(df['Open'], bins=40, alpha=0.7, color='#1abc9c', edgecolor='black')\n",
    "plt.xlabel('Opening Price ($)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "plt.title('Distribution of Opening Prices', fontsize=12, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Box plot of Closing Price by Sentiment--candle chart.\n",
    "print(\"#11. Box plot of Closing Price by Sentiment--candle chart...\")\n",
    "plt.subplot(1,1,1)\n",
    "sns.boxplot(x='Label', y='Close', data=df, palette=colors)\n",
    "plt.xlabel('Sentiment Label', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Closing Price ($)', fontsize=11, fontweight='bold')\n",
    "plt.title('Closing Price by Sentiment', fontsize=12, fontweight='bold')\n",
    "plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistical summaries\n",
    "print(\"\\n=== UNIVARIATE STATISTICS ===\\n\")\n",
    "\n",
    "print(\"1. SENTIMENT LABEL STATISTICS:\")\n",
    "print(df['Label'].describe())\n",
    "\n",
    "print(\"\\n2. STOCK PRICE STATISTICS:\")\n",
    "print(df[['Open', 'High', 'Low', 'Close']].describe())\n",
    "\n",
    "print(\"\\n3. VOLUME STATISTICS:\")\n",
    "print(df['Volume'].describe())\n",
    "\n",
    "print(\"\\n4. NEWS LENGTH STATISTICS:\")\n",
    "print(df['News_Length'].describe())\n",
    "\n",
    "print(\"\\n5. WORD COUNT STATISTICS:\")\n",
    "print(df['Word_Count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hLE0s7OFKilB",
   "metadata": {
    "id": "hLE0s7OFKilB"
   },
   "source": [
    "### **Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yn_9wfzxL-r1",
   "metadata": {
    "id": "Yn_9wfzxL-r1"
   },
   "source": [
    "* Correlation\n",
    "* Sentiment Polarity vs Price\n",
    "* Date vs Price\n",
    "\n",
    "**Note**: The above points are listed to provide guidance on how to approach bivariate analysis. Analysis has to be done beyond the above listed points to get maximum scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- BIVARIATE ANALYSIS ---\\n\")\n",
    "\n",
    "# Create bivariate visualizations\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "print(\"# 1. Correlation Matrix...\")\n",
    "plt.subplot(1,1,1)\n",
    "cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Label', 'News_Length', 'Word_Count']\n",
    "correlation_matrix = df[cols].corr()\n",
    "sns.heatmap(\n",
    "   correlation_matrix, annot=True, vmin=-1, vmax=1,linewidths=0.5, fmt=\".2f\", cmap=\"coolwarm\"\n",
    ")\n",
    "plt.title('Correlation Matrix of Key Variables', fontsize=12, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Label vs Stock Prices (Box Plots)\n",
    "# Create box plots for Open, High, Low, Close prices grouped by Sentiment Label\n",
    "# This will help us know how stock prices vary with sentiment\n",
    "print(\"# 2. Label vs Stock Prices (Box Plots)...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, variable in enumerate(['Open', 'High', 'Low', 'Close']):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.boxplot(data=df, x=\"Label\", y=variable)\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Box plot of Volume by Sentiment\n",
    "#This will help us understand how trading volume varies with sentiment\n",
    "print(\"#3. Box plot of Volume by Sentiment...\")\n",
    "sns.boxplot(\n",
    "    data=df, x=\"Label\", y=\"Volume\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scatter: Volume vs Close Price by Sentiment\n",
    "# This will help us see if there's a relationship between trading volume and closing price, colored by sentiment\n",
    "print(\"# 4. Scatter: Volume vs Close Price by Sentiment...\")\n",
    "plt.subplot(1,1,1)\n",
    "for label, color, name in zip([-1, 0, 1], colors, ['Negative', 'Neutral', 'Positive']):\n",
    "    subset = df[df['Label'] == label]\n",
    "    plt.scatter(subset['Volume'], subset['Close'], c=color, alpha=0.5, \n",
    "                label=name, s=20, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Trading Volume', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Closing Price ($)', fontsize=11, fontweight='bold')\n",
    "plt.title('Volume vs Price by Sentiment', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd54412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Price Change Distribution by Sentiment\n",
    "# This will help us understand how price changes vary with sentiment\n",
    "# And we will know about the impact of stock price on sentiment.\n",
    "print(\"# 5. Price Change Distribution by Sentiment...\")\n",
    "plt.subplot(1,1,1)\n",
    "df_clean = df.dropna(subset=['Price_Change'])\n",
    "sns.boxplot(x='Label', y='Price_Change', data=df_clean, palette=colors)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Sentiment Label', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Price Change (%)', fontsize=11, fontweight='bold')\n",
    "plt.title('Price Change Distribution by Sentiment', fontsize=12, fontweight='bold')\n",
    "plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ac5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. News Length vs Sentiment vs Volume\n",
    "# This will help us see how news length relates to trading volume, colored by sentiment\n",
    "# And we will know about the impact of news length on trading volume.\n",
    "print(\"# 6. News Length vs Sentiment vs Volume...\")\n",
    "plt.subplot(1,1,1)\n",
    "for label, color, name in zip([-1, 0, 1], colors, ['Negative', 'Neutral', 'Positive']):\n",
    "    subset = df[df['Label'] == label]\n",
    "    plt.scatter(subset['News_Length'], subset['Volume'], c=color, \n",
    "                alpha=0.5, label=name, s=30)\n",
    "plt.xlabel('News Length (characters)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Trading Volume', fontsize=11, fontweight='bold')\n",
    "plt.title('News Length vs Volume by Sentiment', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Additional bivariate analysis\n",
    "# Create additional bivariate visualizations\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# This will help us understand relationships between sentiment, stock prices, volume, and time.\n",
    "# And we will know about the impact of stock prices, volume, and time on sentiment.\n",
    "print(\"# 7. Additional bivariate analysis- relationships between sentiment, stock prices, volume, and time...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Average closing price by sentiment\n",
    "axes[0, 0].bar([-1, 0, 1], df.groupby('Label')['Close'].mean(), \n",
    "               color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Sentiment Label', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Average Closing Price ($)', fontweight='bold')\n",
    "axes[0, 0].set_title('Average Closing Price by Sentiment', fontweight='bold')\n",
    "axes[0, 0].set_xticks([-1, 0, 1])\n",
    "axes[0, 0].set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "# Average volume by sentiment\n",
    "axes[0, 1].bar([-1, 0, 1], df.groupby('Label')['Volume'].mean(), \n",
    "               color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Sentiment Label', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Average Trading Volume', fontweight='bold')\n",
    "axes[0, 1].set_title('Average Trading Volume by Sentiment', fontweight='bold')\n",
    "axes[0, 1].set_xticks([-1, 0, 1])\n",
    "axes[0, 1].set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "axes[0, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Price range by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_price = df.groupby('Day_of_Week')['Price_Range'].mean().reindex(day_order).dropna()\n",
    "axes[1, 0].bar(range(len(day_price)), day_price.values, alpha=0.8, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Day of Week', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Price Range ($)', fontweight='bold')\n",
    "axes[1, 0].set_title('Average Price Range by Day of Week', fontweight='bold')\n",
    "axes[1, 0].set_xticks(range(len(day_price)))\n",
    "axes[1, 0].set_xticklabels(day_price.index, rotation=45)\n",
    "\n",
    "# Sentiment count by month\n",
    "monthly_sentiment = df.groupby(['Month', 'Label']).size().unstack(fill_value=0)\n",
    "monthly_sentiment.plot(kind='bar', stacked=True, color=colors, \n",
    "                       alpha=0.8, ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Month', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count', fontweight='bold')\n",
    "axes[1, 1].set_title('Monthly Sentiment Distribution', fontweight='bold')\n",
    "axes[1, 1].legend(['Negative', 'Neutral', 'Positive'])\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation insights\n",
    "print(\"=== BIVARIATE STATISTICS ===\\n\")\n",
    "\n",
    "print(\"1. CORRELATION WITH SENTIMENT:\")\n",
    "print(f\"   - Close Price: {df['Label'].corr(df['Close']):.4f}\")\n",
    "print(f\"   - Volume: {df['Label'].corr(df['Volume']):.4f}\")\n",
    "print(f\"   - News Length: {df['Label'].corr(df['News_Length']):.4f}\")\n",
    "\n",
    "print(\"\\n2. AVERAGE METRICS BY SENTIMENT:\")\n",
    "sentiment_stats = df.groupby('Label')[['Close', 'Volume', 'News_Length', 'Word_Count']].mean()\n",
    "print(sentiment_stats)\n",
    "\n",
    "print(\"\\n3. PRICE CHANGE BY SENTIMENT:\")\n",
    "if 'Price_Change' in df.columns:\n",
    "    price_change_stats = df.groupby('Label')['Price_Change'].agg(['mean', 'median', 'std'])\n",
    "    print(price_change_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== KEY OBSERVATIONS FROM EDA ===\")\n",
    "print(\"\"\"\n",
    "1. SENTIMENT PATTERNS:\n",
    "    - Positive sentiment dominates the dataset, followed by negative and very few neutral samples.\n",
    "    - Sentiment labels show weak linear correlation with numeric features, suggesting text features are more informative.\n",
    "\n",
    "2. STOCK PRICE PATTERNS:\n",
    "    - Stock prices (Open, High, Low, Close) are highly correlated, indicating similar daily movement.\n",
    "    - Price volatility (Price_Range) and trading volume show spikes that often align with major news events.\n",
    "\n",
    "3. NEWS CHARACTERISTICS:\n",
    "    - News articles vary widely in length and word count; longer articles may indicate more significant events.\n",
    "    - News length and word count are strongly correlated.\n",
    "\n",
    "4. ACTIONABLE INSIGHTS:\n",
    "    - Focus on text embeddings for sentiment prediction, as numeric features alone are insufficient.\n",
    "    - Engineer features such as price change, price range, and log-transformed volume for modeling.\n",
    "    - Address class imbalance due to few neutral samples.\n",
    "    - Align major news dates with price/volume spikes for deeper analysis.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N8z4-vOBmwqv",
   "metadata": {
    "id": "N8z4-vOBmwqv"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U3JRcGfghIgy",
   "metadata": {
    "id": "U3JRcGfghIgy"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Lets check the date column and its statistics\n",
    "df['Date'].describe()\n",
    "\n",
    "# Text preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by converting to lowercase, removing punctuation,\n",
    "    and extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'News' column\n",
    "print(\"Preprocessing news text...\")\n",
    "df['Processed_News'] = df['News'].progress_apply(preprocess_text)\n",
    "print(\"----- Text preprocessing completed------\\n\")\n",
    "\n",
    "# Display examples\n",
    "print(\"Sample of original vs preprocessed text:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original: {df['News'].iloc[i][:150]}...\")\n",
    "    print(f\"Processed: {df['Processed_News'].iloc[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "print(\"\\nSplitting features and target variable...\")\n",
    "X = df['Processed_News']\n",
    "y = df['Label']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d90c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTraining set label distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nTest set label distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rYgR14ORf7b",
   "metadata": {
    "id": "0rYgR14ORf7b"
   },
   "source": [
    "## **Word Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORD EMBEDDINGS\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4IUBFAOTbjju",
   "metadata": {
    "id": "4IUBFAOTbjju"
   },
   "source": [
    "#### **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d843dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- WORD2VEC EMBEDDINGS -------------------\n",
    "print(\"--- WORD2VEC EMBEDDINGS ---\\n\")\n",
    "\n",
    "# Tokenize for Word2Vec\n",
    "print(\"Tokenizing text for Word2Vec...\")\n",
    "tokenized_train = [text.split() for text in X_train]\n",
    "tokenized_test = [text.split() for text in X_test]\n",
    "\n",
    "# Display first 2 tokenized examples\n",
    "tokenized_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Word2Vec model trained in {training_time:.2f} seconds\")\n",
    "print(f\"Vocabulary size: {len(word2vec_model.wv)}\")\n",
    "\n",
    "# List of words in the vocabulary\n",
    "print(word2vec_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get average word vector for a text\n",
    "def get_avg_word_vector(tokens, model):\n",
    "    \"\"\"\n",
    "    Get average word vector for a list of tokens\n",
    "    Returns zero vector if no words are in vocabulary\n",
    "    \"\"\"\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "# Generate Word2Vec embeddings\n",
    "print(\"\\nGenerating Word2Vec embeddings...\")\n",
    "\n",
    "# Apply the function directly to tokenized data\n",
    "embeddings_train = [get_avg_word_vector(tokens, word2vec_model) for tokens in tqdm(tokenized_train, desc=\"Train embeddings\")]\n",
    "embeddings_test = [get_avg_word_vector(tokens, word2vec_model) for tokens in tqdm(tokenized_test, desc=\"Test embeddings\")]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_w2v = np.array(embeddings_train)\n",
    "X_test_w2v = np.array(embeddings_test)\n",
    "print(f\"Word2Vec Embeddings - Training set shape: {X_train_w2v.shape}\")\n",
    "print(f\"Word2Vec Embeddings - Test set shape: {X_test_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3GUvne0hyPx",
   "metadata": {
    "id": "a3GUvne0hyPx"
   },
   "source": [
    "### **Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- SENTENCE TRANSFORMERS EMBEDDINGS -------------------\n",
    "print(\"\\n--- SENTENCE TRANSFORMERS EMBEDDINGS ---\\n\")\n",
    "# Load pre-trained Sentence Transformer model\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "model_sentf = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Sentence Transformer model loaded.\")\n",
    "\n",
    "# Generate embeddings in batches for efficiency\n",
    "print(\"\\nGenerating Sentence Transformer embeddings...\")\n",
    "batch_size = 32\n",
    "print(\"\\nGenerating Sentence Transformer embeddings...\")\n",
    "start_time = time.time()\n",
    "X_train_st = model_sentf.encode(\n",
    "    X_train.tolist(), \n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "X_test_st = model_sentf.encode(\n",
    "    X_test.tolist(),\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "encoding_time = time.time() - start_time\n",
    "\n",
    "print(f\"Encoding completed in {encoding_time:.2f} seconds\")\n",
    "print(f\"Train embeddings shape: {X_train_st.shape}\")\n",
    "print(f\"Test embeddings shape: {X_test_st.shape}\")\n",
    "\n",
    "import gc\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n All embeddings generated successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4lwYN5bYmHp",
   "metadata": {
    "id": "f4lwYN5bYmHp"
   },
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AY78gPn0i2rU",
   "metadata": {
    "id": "AY78gPn0i2rU"
   },
   "source": [
    "**Note:**  \n",
    "You can use the helper functions provided below to:\n",
    "- Plot a **confusion matrix** (`plot_confusion_matrix`)\n",
    "- Generate key **classification metrics** like accuracy, recall, precision, and F1-score (`model_performance_classification_sklearn`)\n",
    "\n",
    "These are ready-to-use. However, you’re welcome to explore and write your own evaluation code if you prefer. Feel free to modify or extend these as per your learning goals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZkQO_WCCp-pq",
   "metadata": {
    "id": "ZkQO_WCCp-pq"
   },
   "source": [
    "##### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TmpiQb6ohfFv",
   "metadata": {
    "id": "TmpiQb6ohfFv"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix to visualize the performance of a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    actual (array-like): The true labels.\n",
    "    predicted (array-like): The predicted labels from the model.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the confusion matrix.\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "    # Create a new figure with a specified size\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    # Define the labels for the confusion matrix dynamically from the data\n",
    "    label_list = sorted(list(np.unique(np.concatenate((actual, predicted)))))\n",
    "\n",
    "    # Plot the confusion matrix using a heatmap with annotations\n",
    "    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues', xticklabels=label_list, yticklabels=label_list)\n",
    "\n",
    "    # Label for the y-axis\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    # Label for the x-axis\n",
    "    plt.xlabel('Predicted')\n",
    "\n",
    "    # Title of the plot\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kGJsSn_SKQpP",
   "metadata": {
    "id": "kGJsSn_SKQpP"
   },
   "outputs": [],
   "source": [
    "def model_performance_classification_sklearn(actual, predicted):\n",
    "    \"\"\"\n",
    "    Compute various performance metrics for a classification model using sklearn.\n",
    "\n",
    "    Parameters:\n",
    "    model (sklearn classifier): The classification model to evaluate.\n",
    "    predictors (array-like): The independent variables used for predictions.\n",
    "    target (array-like): The true labels for the dependent variable.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the computed metrics (Accuracy, Recall, Precision, F1-score).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute Accuracy\n",
    "    acc = accuracy_score(actual,predicted)\n",
    "    # Compute Recall\n",
    "    recall = recall_score(actual,predicted,average='weighted')\n",
    "    # Compute Precision\n",
    "    precision = precision_score(actual,predicted,average='weighted')\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(actual,predicted,average='weighted')\n",
    "\n",
    "    # Create a DataFrame to store the computed metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": [acc],\n",
    "            \"Recall\": [recall],\n",
    "            \"Precision\": [precision],\n",
    "            \"F1\": [f1],\n",
    "        }\n",
    "    )\n",
    "    # Return the DataFrame with the metrics\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODEL EVALUATION CRITERION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION CRITERION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "PRIMARY EVALUATION METRIC: F1-SCORE (Weighted Average)\n",
    "\n",
    "RATIONALE:\n",
    "1. BALANCED PERFORMANCE: F1-score provides a balance between Precision and Recall,\n",
    "   which is crucial for sentiment analysis where both false positives and false\n",
    "   negatives are important.\n",
    "\n",
    "2. HANDLING CLASS IMBALANCE: Weighted F1-score accounts for class imbalance by\n",
    "   considering the support (number of samples) for each class, making it ideal\n",
    "   when sentiment classes are not equally distributed.\n",
    "\n",
    "3. BUSINESS RELEVANCE: For investment decisions, we need both:\n",
    "   - High Precision: To avoid false positive signals that could lead to poor investments\n",
    "   - High Recall: To not miss important sentiment signals that could affect decisions\n",
    "\n",
    "SECONDARY METRICS:\n",
    "- Accuracy: Overall correctness of predictions\n",
    "- Precision: How many predicted sentiments are actually correct\n",
    "- Recall: How many actual sentiments are correctly identified\n",
    "\n",
    "All models will be compared using these metrics, with F1-score as the primary\n",
    "criterion for final model selection.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqg3KJ6TClm1",
   "metadata": {
    "id": "pqg3KJ6TClm1"
   },
   "source": [
    "### **Build Random Forest Models using text embeddings from Word2Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "STLzE_VviKQX",
   "metadata": {
    "id": "STLzE_VviKQX"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS - RANDOM FOREST MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Store results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# ------------------- RF with Word2Vec -------------------\n",
    "print(\"--- MODEL 1: Random Forest + Word2Vec ---\\n\")\n",
    "\n",
    "print(\"Training Random Forest with Word2Vec embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_w2v = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_w2v.fit(X_train_w2v, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred_rf_w2v = rf_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "rf_w2v_metrics = model_performance_classification_sklearn(y_test, y_pred_rf_w2v)\n",
    "rf_w2v_metrics['Model'] = 'Random Forest + Word2Vec'\n",
    "rf_w2v_metrics['Training_Time'] = training_time\n",
    "display(rf_w2v_metrics)\n",
    "\n",
    "results_df = pd.concat([results_df, rf_w2v_metrics], ignore_index=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "plot_confusion_matrix(y_test, y_pred_rf_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- RF with Sentence Transformer -------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"--- MODEL 2: Random Forest + Sentence Transformer ---\\n\")\n",
    "\n",
    "print(\"Training Random Forest with Sentence Transformer embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_st = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_st.fit(X_train_st, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred_rf_st = rf_st.predict(X_test_st)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "rf_st_metrics = model_performance_classification_sklearn(y_test, y_pred_rf_st)\n",
    "rf_st_metrics['Model'] = 'Random Forest + Sentence Transformer'\n",
    "rf_st_metrics['Training_Time'] = training_time\n",
    "display(rf_st_metrics)\n",
    "\n",
    "results_df = pd.concat([results_df, rf_st_metrics], ignore_index=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "plot_confusion_matrix(y_test, y_pred_rf_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DHgj_cCm2pIn",
   "metadata": {
    "id": "DHgj_cCm2pIn"
   },
   "source": [
    "### **Building Neural Network Models using different text embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4rqA7vPBiLnj",
   "metadata": {
    "id": "4rqA7vPBiLnj"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SENTIMENT ANALYSIS - NEURAL NETWORK MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Prepare data for neural networks\n",
    "print(\"Preparing data for neural networks...\\n\")\n",
    "\n",
    "# Convert labels: -1 → 0, 0 → 1, 1 → 2\n",
    "# Neural networks need labels starting from 0\n",
    "label_mapping = {1: 2, -1: 0, 0: 1}\n",
    "y_train_mapped_wv = [label_mapping[label] for label in y_train]\n",
    "y_test_mapped_wv = [label_mapping[label] for label in y_test]\n",
    "\n",
    "# Convert features to NumPy arrays\n",
    "X_train_wv_np = np.array(X_train_w2v)\n",
    "X_test_wv_np = np.array(X_test_w2v)\n",
    "y_train_mapped_wv = np.array(y_train_mapped_wv)\n",
    "y_test_mapped_wv = np.array(y_test_mapped_wv)\n",
    "\n",
    "print(f\"X_train_w2v shape: {X_train_wv_np.shape}\")\n",
    "print(f\"X_test_w2v shape: {X_test_wv_np.shape}\")\n",
    "print(f\"y_train_mapped shape: {y_train_mapped_wv.shape}\")\n",
    "print(f\"y_test_mapped shape: {y_test_mapped_wv.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()    \n",
    "\n",
    "# Clear any previous TensorFlow/Keras sessions from memory (recommended when re-running cells)\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "# ------------------- NN with Word2Vec -------------------\n",
    "print(\"--- MODEL 3: Neural Network + Word2Vec ---\\n\")\n",
    "print(\"Building Neural Network architecture...\")\n",
    "# Define the model architecture\n",
    "nn_w2v = Sequential()\n",
    "\n",
    "# Input layer:\n",
    "nn_w2v.add(Dense(128, activation='relu', input_shape=(X_train_wv_np.shape[1],)))  # Input shape = size of Word2Vec embeddings\n",
    "# Dropout layer:\n",
    "nn_w2v.add(Dropout(0.3))\n",
    "# Hidden layer:\n",
    "nn_w2v.add(Dense(64, activation='relu'))\n",
    "# Hidden Layer:\n",
    "nn_w2v.add(Dense(32, activation='relu'))\n",
    "# Output layer (as classification with 3 classes):\n",
    "nn_w2v.add(Dense(3, activation='softmax'))\n",
    "# Compile the model\n",
    "# optimizer: Choosen 'adam' - other alternatives ->'sgd', or 'rmsprop'\n",
    "nn_w2v.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Print the model summary\n",
    "nn_w2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908242b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\nTraining Neural Network with Word2Vec embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_w2v = nn_w2v.fit(\n",
    "    X_train_wv_np, \n",
    "    y_train_mapped_wv,\n",
    "    validation_data=(X_test_wv_np, y_test_mapped_wv),\n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nModel trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions on training data...\")\n",
    "\n",
    "# Predict class probabilities on training data\n",
    "y_train_pred_probs = nn_w2v.predict(X_train_wv_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_preds = tf.argmax(y_train_pred_probs, axis=1).numpy()\n",
    "\n",
    "# Predict class probabilities on test data\n",
    "y_test_pred_probs = nn_w2v.predict(X_test_wv_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_test_preds = tf.argmax(y_test_pred_probs, axis=1).numpy()\n",
    "print(\"Predictions completed.\")\n",
    "\n",
    "# Convert back to [-1, 0, 1] to match utility function expectations\n",
    "label_mapping = {2: 1, 0: -1, 1: 0}\n",
    "y_train_preds = np.array([label_mapping[index] for index in y_train_preds])\n",
    "y_test_preds = np.array([label_mapping[index] for index in y_test_preds])\n",
    "\n",
    "plot_confusion_matrix(y_train, y_train_preds)\n",
    "plot_confusion_matrix(y_test, y_test_preds)\n",
    "\n",
    "#Calculating different metrics on training data\n",
    "NN_train_wv = model_performance_classification_sklearn(y_train,y_train_preds)\n",
    "print(\"Training performance:\\n\", NN_train_wv)\n",
    "#Calculating different metrics on test data\n",
    "NN_test_wv = model_performance_classification_sklearn(y_test,y_test_preds)\n",
    "print(\"Test performance:\\n\", NN_test_wv)\n",
    "\n",
    "\n",
    "NN_test_wv['Model'] = 'Neural Network + Word2Vec'\n",
    "\n",
    "NN_test_wv['Training_Time'] = training_time\n",
    "results_df = pd.concat([results_df, NN_test_wv], ignore_index=True) \n",
    "\n",
    "results_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d55302",
   "metadata": {},
   "source": [
    "#### **Building a Neural Network Model using text embeddings obtained from the Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels\n",
    "label_mapping = {1: 2, -1: 0, 0: 1}\n",
    "y_train_mapped_st = [label_mapping[label] for label in y_train]\n",
    "y_test_mapped_st = [label_mapping[label] for label in y_test]\n",
    "\n",
    "# Convert your features DataFrame to a NumPy array\n",
    "X_train_st_np = np.array(X_train_st)\n",
    "X_test_st_np = np.array(X_test_st)\n",
    "y_train_mapped_st = np.array(y_train_mapped_st)\n",
    "y_test_mapped_st = np.array(y_test_mapped_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any previous TensorFlow/Keras sessions from memory (recommended when re-running cells)\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Define the model architecture\n",
    "nn_st = Sequential()\n",
    "\n",
    "# Input layer:\n",
    "nn_st.add(Dense(128, activation='relu', input_shape=(X_train_st.shape[1],))) # Input shape = size of Sentence Transformer embeddings\n",
    "# Dropout layer:\n",
    "nn_st.add(Dropout(0.3))\n",
    "# Hidden layer:\n",
    "nn_st.add(Dense(64, activation='relu'))\n",
    "# Hidden layer:\n",
    "nn_st.add(Dense(32, activation='relu'))\n",
    "# Output layer:\n",
    "# 3 output classes → use softmax for multi-class classification\n",
    "nn_st.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "nn_st.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "nn_st.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722faafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training data\n",
    "print(\"\\nTraining Neural Network with Sentence Transformer embeddings...\")\n",
    "start_time = time.time()\n",
    "history = nn_st.fit(\n",
    "    X_train_st_np, y_train_mapped_st,\n",
    "    validation_data=(X_test_st_np, y_test_mapped_st),\n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nModel trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Predict class probabilities on training data\n",
    "y_train_pred_probs = nn_st.predict(X_train_st_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_preds = tf.argmax(y_train_pred_probs, axis=1).numpy()\n",
    "\n",
    "# Predict class probabilities on test data\n",
    "y_test_pred_probs = nn_st.predict(X_test_st_np)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_test_preds = tf.argmax(y_test_pred_probs, axis=1).numpy()\n",
    "print(\"Predictions completed.\")\n",
    "\n",
    "# Convert back to [-1, 0, 1] to match utility function expectations\n",
    "label_mapping = {2: 1, 0: -1, 1: 0}\n",
    "y_train_preds = np.array([label_mapping[index] for index in y_train_preds])\n",
    "y_test_preds = np.array([label_mapping[index] for index in y_test_preds])\n",
    "\n",
    "plot_confusion_matrix(y_train, y_train_preds)\n",
    "plot_confusion_matrix(y_test, y_test_preds)\n",
    "#Calculating different metrics on training data\n",
    "NN_train_st = model_performance_classification_sklearn(y_train,y_train_preds)\n",
    "print(\"Training performance:\\n\", NN_train_st)\n",
    "#Calculating different metrics on test data\n",
    "NN_test_st = model_performance_classification_sklearn(y_test,y_test_preds)\n",
    "print(\"Test performance:\\n\", NN_test_st)\n",
    "NN_test_st['Model'] = 'Neural Network + Sentence Transformer'\n",
    "NN_test_st['Training_Time'] = training_time\n",
    "results_df = pd.concat([results_df, NN_test_st], ignore_index=True)\n",
    "results_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gsmrYpkrFY2A",
   "metadata": {
    "id": "gsmrYpkrFY2A"
   },
   "source": [
    "### **Model Performance Summary and Final Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q9LcgdCciUX5",
   "metadata": {
    "id": "q9LcgdCciUX5"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "# Display all results\n",
    "results_df = results_df[['Model', 'Accuracy', 'Recall', 'Precision', 'F1', 'Training_Time']]\n",
    "results_df = results_df.sort_values(by='F1', ascending=False).reset_index(drop=True)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HiOLoD7BO3L-",
   "metadata": {
    "id": "HiOLoD7BO3L-"
   },
   "source": [
    "## **Conclusions and Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CvLbKjeoiabi",
   "metadata": {
    "id": "CvLbKjeoiabi"
   },
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
