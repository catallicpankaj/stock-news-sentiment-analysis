{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c1613d",
   "metadata": {},
   "source": [
    "## ROOT CAUSES OF LIMITED PERFORMANCE (~72% Accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. CLASS IMBALANCE PROBLEM ‚ö†Ô∏è\n",
    "- **Positive sentiment dominates the dataset**\n",
    "- **Very few neutral samples** (class imbalance noted in observations)\n",
    "- Models struggle to learn minority classes effectively\n",
    "- This explains why precision/recall may be uneven across classes\n",
    "\n",
    "---\n",
    "\n",
    "### 2. INSUFFICIENT TEXT PREPROCESSING\n",
    "**Current preprocessing only does:**\n",
    "- Lowercase conversion\n",
    "- Punctuation removal\n",
    "- Whitespace normalization\n",
    "\n",
    "**Missing critical steps:**\n",
    "- No stopword removal (common words add noise)\n",
    "- No lemmatization/stemming (reduces vocabulary sparsity)\n",
    "- No handling of special characters, URLs, numbers\n",
    "- No removal of extremely short/long outlier texts\n",
    "\n",
    "---\n",
    "\n",
    "### 3. WEAK CORRELATIONS WITH NUMERIC FEATURES\n",
    "- Sentiment shows weak linear correlation with stock prices, volume, news length\n",
    "- The notebook correctly identifies: \"text features are more informative\"\n",
    "- Yet the model only uses text embeddings - no feature engineering was performed\n",
    "\n",
    "---\n",
    "\n",
    "### 4. SUBOPTIMAL EMBEDDINGS\n",
    "**Word2Vec Issues:**\n",
    "- Small training corpus ‚Üí poor quality embeddings\n",
    "- `min_count=2` may exclude rare but important words\n",
    "- Simple averaging of word vectors loses word order/context\n",
    "- `vector_size=100` may be too small for complex sentiment nuances\n",
    "\n",
    "**Sentence Transformer:**\n",
    "- all-MiniLM-L6-v2 is a general-purpose model\n",
    "- Not fine-tuned for financial sentiment (domain-specific)\n",
    "- May not capture market-specific terminology\n",
    "\n",
    "---\n",
    "\n",
    "### 5. MODEL ARCHITECTURE LIMITATIONS\n",
    "**Random Forest:**\n",
    "- `max_depth=5` is very shallow for complex text patterns\n",
    "- May be underfitting\n",
    "- No hyperparameter tuning performed\n",
    "\n",
    "**Neural Network:**\n",
    "- Simple architecture (3 layers)\n",
    "- No regularization besides dropout\n",
    "- Fixed `epochs=30` without early stopping\n",
    "- No learning rate scheduling\n",
    "- Validation data only used for monitoring, not for preventing overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## PRACTICAL RECOMMENDATIONS TO IMPROVE BEYOND 72%\n",
    "\n",
    "### Priority 1: Address Class Imbalance üéØ\n",
    "- Use class weights in models:\n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "```\n",
    "\n",
    "- Apply SMOTE for oversampling minority classes:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_embeddings, y_train)\n",
    "```\n",
    "- Use stratified sampling (already done - good!)\n",
    "\n",
    "---\n",
    "\n",
    "### Priority 2: Enhanced Text Preprocessing üìù\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def enhanced_preprocess(text):\n",
    "    # Existing steps...\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # NEW: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # NEW: Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "```\n",
    "---\n",
    "\n",
    "### Priority 3: Feature Engineering üîß\n",
    "- Create hybrid features combining text + numeric signals:\n",
    "```python\n",
    "additional_features = df[['Price_Change', 'Price_Range', 'Volume', 'News_Length', 'Word_Count']].fillna(0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features)\n",
    "X_train_hybrid = np.concatenate([X_train_st, additional_features_scaled_train], axis=1)\n",
    "X_test_hybrid = np.concatenate([X_test_st, additional_features_scaled_test], axis=1)\n",
    "```\n",
    "---\n",
    "\n",
    "### Priority 4: Better Embeddings üöÄ\n",
    "- **Option A: Domain-specific model**\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_sentf = SentenceTransformer('ProsusAI/finbert')\n",
    "# or\n",
    "model_sentf = SentenceTransformer('yiyanghkust/finbert-tone')\n",
    "```\n",
    "- **Option B: Improved Word2Vec**\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=300,  # Increase from 100\n",
    "    window=10,        # Increase context window\n",
    "    min_count=1,      # Don't exclude rare words\n",
    "    sg=1,\n",
    "    epochs=20,        # More training\n",
    "    seed=42\n",
    ")\n",
    "```\n",
    "---\n",
    "\n",
    "### Priority 5: Model Architecture Improvements ‚öôÔ∏è\n",
    "- **Random Forest:**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],  # Increase depth!\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced']  # Handle imbalance\n",
    "}\n",
    "rf_tuned = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1_weighted')\n",
    "rf_tuned.fit(X_train_st, y_train)\n",
    "```\n",
    "\n",
    "- **Neural Network:**\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "nn_improved = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_st.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "nn_improved.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "history = nn_improved.fit(\n",
    "    X_train_st_np, y_train_mapped_st,\n",
    "    validation_data=(X_test_st_np, y_test_mapped_st),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,  # Add class weights!\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "```\n",
    "---\n",
    "\n",
    "### Priority 6: Try Advanced Models üèÜ\n",
    "- **Ensemble approach**\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),  # Handle imbalance\n",
    "    random_state=42\n",
    ")\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('rf', rf_tuned.best_estimator_), ('xgb', xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "```\n",
    "---\n",
    "\n",
    "## EXPECTED IMPROVEMENTS\n",
    "- Implementing these recommendations should improve metrics to:\n",
    "    - **Accuracy:** 78-85% (from 72%)\n",
    "    - **F1-Score:** 0.78-0.84 (balanced across classes)\n",
    "    - **Recall for minority class:** +15-20%\n",
    "\n",
    "**Implementation Priority Order:**\n",
    "- ‚úÖ Class imbalance handling (biggest impact)\n",
    "- ‚úÖ Enhanced preprocessing\n",
    "- ‚úÖ Hyperparameter tuning for RF\n",
    "- ‚úÖ Domain-specific embeddings (FinBERT)\n",
    "- ‚úÖ Feature engineering (hybrid approach)\n",
    "- ‚úÖ Improved NN architecture with callbacks\n",
    "\n",
    "---\n",
    "\n",
    "**Would you like me to implement any of these improvements in your notebook?**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
